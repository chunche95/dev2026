NAMESPACE              LAST SEEN   TYPE      REASON                              OBJECT                                                         MESSAGE
monitoring             34m         Normal    Scheduled                           pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Successfully assigned monitoring/prometheus-monitorgrafana-kube-promet-prometheus-0 to debianworker2
default                3m50s       Normal    Starting                            node/debianworker2                                             
default                34m         Normal    Starting                            node/debianworker2                                             
default                34m         Normal    Starting                            node/debianmaster                                              
default                74d         Normal    Starting                            node/debianworker2                                             
argocd                 34m         Normal    Scheduled                           pod/argocd-application-controller-0                            Successfully assigned argocd/argocd-application-controller-0 to debianworker2
default                16m         Normal    Starting                            node/debianworker1                                             
default                103s        Normal    Starting                            node/debianworker1                                             
default                74d         Normal    Starting                            node/debianworker1                                             
default                18m         Normal    Starting                            node/debianworker2                                             
default                <invalid>   Normal    Starting                            node/debianworker1                                             
monitoring             34m         Normal    Scheduled                           pod/loki-0                                                     Successfully assigned monitoring/loki-0 to debianworker2
default                74d         Normal    CertificateExpirationOK             node/debianworker2                                             Node and Certificate Authority certificates managed by k3s are OK
default                74d         Normal    NodeHasSufficientPID                node/debianworker2                                             Node debianworker2 status is now: NodeHasSufficientPID
default                74d         Normal    Starting                            node/debianworker2                                             Starting kubelet.
default                74d         Warning   InvalidDiskCapacity                 node/debianworker2                                             invalid capacity 0 on image filesystem
default                74d         Normal    NodeHasSufficientMemory             node/debianworker2                                             Node debianworker2 status is now: NodeHasSufficientMemory
default                74d         Normal    NodeAllocatableEnforced             node/debianworker2                                             Updated Node Allocatable limit across pods
default                74d         Normal    NodeHasNoDiskPressure               node/debianworker2                                             Node debianworker2 status is now: NodeHasNoDiskPressure
default                74d         Warning   PossibleMemoryBackedVolumesOnDisk   node/debianworker2                                             The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
cyclops                74d         Warning   Unhealthy                           pod/cyclops-ctrl-8685b986c7-tmr5r                              Readiness probe failed: Get "http://10.42.2.147:8082/readyz": dial tcp 10.42.2.147:8082: i/o timeout (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/traefik-5cbdcf97f4-bbnx7                                   Liveness probe failed: Get "http://10.42.2.129:9000/ping": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/kite-7cb79df49c-nth8h                                      Readiness probe failed: Get "http://10.42.2.140:8080/healthz": dial tcp 10.42.2.140:8080: i/o timeout (Client.Timeout exceeded while awaiting headers)
kubernetes-dashboard   74d         Warning   Unhealthy                           pod/kubernetes-dashboard-8696f5f494-js7b8                      Liveness probe failed: Get "https://10.42.2.137:8443/": net/http: TLS handshake timeout
monitoring             74d         Normal    Pulled                              pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Container image "quay.io/prometheus-operator/prometheus-operator:v0.85.0" already present on machine
kube-system            74d         Warning   Failed                              pod/kite-7cb79df49c-nth8h                                      Error: failed to sync secret cache: timed out waiting for the condition
kube-system            74d         Warning   Unhealthy                           pod/traefik-5cbdcf97f4-bbnx7                                   Readiness probe failed: Get "http://10.42.2.129:9000/ping": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Normal    Killing                             pod/traefik-5cbdcf97f4-bbnx7                                   Container traefik failed liveness probe, will be restarted
monitoring             74d         Normal    Created                             pod/monitorgrafana-prometheus-node-exporter-8dgb6              Created container: node-exporter
cyclops                74d         Normal    Killing                             pod/cyclops-ctrl-8685b986c7-tmr5r                              Container cyclops-ctrl failed liveness probe, will be restarted
monitoring             74d         Normal    Created                             pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Created container: kube-prometheus-stack
monitoring             74d         Normal    Started                             pod/monitorgrafana-prometheus-node-exporter-8dgb6              Started container node-exporter
kube-system            74d         Normal    Pulled                              pod/kite-7cb79df49c-nth8h                                      Container image "ghcr.io/zxh326/kite:latest" already present on machine
monitoring             74d         Normal    Started                             pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Started container kube-prometheus-stack
cyclops                74d         Normal    Pulled                              pod/cyclops-ctrl-8685b986c7-tmr5r                              Container image "cyclopsui/cyclops-ctrl:v0.21.1" already present on machine
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Liveness probe failed: Get "https://10.42.2.145:10250/healthz": dial tcp 10.42.2.145:10250: connect: connection refused
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Readiness probe failed: Get "https://10.42.2.145:10250/healthz": dial tcp 10.42.2.145:10250: connect: connection refused
argocd                 74d         Normal    Killing                             pod/argocd-repo-server-95c9d69c6-2vxj4                         Container argocd-repo-server failed liveness probe, will be restarted
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Liveness probe failed: Get "http://10.42.2.134:8080/livez": dial tcp 10.42.2.134:8080: connect: connection refused
argocd                 74d         Normal    Pulling                             pod/argocd-repo-server-95c9d69c6-2vxj4                         Pulling image "quay.io/argoproj/argocd:v3.1.1"
kube-system            74d         Normal    Created                             pod/traefik-5cbdcf97f4-bbnx7                                   Created container: traefik
cyclops                74d         Normal    Created                             pod/cyclops-ctrl-8685b986c7-tmr5r                              Created container: cyclops-ctrl
kube-system            74d         Normal    Created                             pod/kite-7cb79df49c-nth8h                                      Created container: kite
kube-system            74d         Normal    Started                             pod/traefik-5cbdcf97f4-bbnx7                                   Started container traefik
cyclops                74d         Normal    Started                             pod/cyclops-ctrl-8685b986c7-tmr5r                              Started container cyclops-ctrl
kube-system            74d         Normal    Started                             pod/kite-7cb79df49c-nth8h                                      Started container kite
argocd                 74d         Normal    Pulled                              pod/argocd-repo-server-95c9d69c6-2vxj4                         Successfully pulled image "quay.io/argoproj/argocd:v3.1.1" in 19.576s (19.576s including waiting). Image size: 191031950 bytes.
argocd                 74d         Normal    Created                             pod/argocd-repo-server-95c9d69c6-2vxj4                         Created container: argocd-repo-server
argocd                 74d         Normal    Started                             pod/argocd-repo-server-95c9d69c6-2vxj4                         Started container argocd-repo-server
kube-system            74d         Warning   Unhealthy                           pod/traefik-5cbdcf97f4-bbnx7                                   Liveness probe failed: Get "http://10.42.2.129:9000/ping": dial tcp 10.42.2.129:9000: connect: connection refused
kube-system            74d         Warning   Unhealthy                           pod/kite-7cb79df49c-nth8h                                      Readiness probe failed: Get "http://10.42.2.140:8080/healthz": dial tcp 10.42.2.140:8080: connect: connection refused
cyclops                74d         Warning   Unhealthy                           pod/cyclops-ctrl-8685b986c7-tmr5r                              Readiness probe failed: Get "http://10.42.2.147:8082/readyz": dial tcp 10.42.2.147:8082: connect: connection refused
kube-system            74d         Warning   Unhealthy                           pod/kite-7cb79df49c-nth8h                                      Liveness probe failed: Get "http://10.42.2.140:8080/healthz": dial tcp 10.42.2.140:8080: connect: connection refused
argocd                 74d         Warning   Unhealthy                           pod/argocd-repo-server-95c9d69c6-2vxj4                         Readiness probe failed: Get "http://10.42.2.142:8084/healthz": dial tcp 10.42.2.142:8084: connect: connection refused
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Readiness probe failed: Get "https://10.42.2.145:10250/healthz": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/metrics-server-7bf7d58749-v5vbk                            Readiness probe failed: Get "https://10.42.2.124:10250/readyz": context deadline exceeded
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-677d8d6465-tktwr                            Liveness probe failed: Get "http://10.42.2.118:3000/api/health": dial tcp 10.42.2.118:3000: connect: connection refused
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-prometheus-node-exporter-8dgb6              Readiness probe failed: Get "http://192.168.0.18:9100/": dial tcp 192.168.0.18:9100: i/o timeout (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/coredns-ccb96694c-2mrz9                                    Liveness probe failed: Get "http://10.42.2.125:8080/health": dial tcp 10.42.2.125:8080: i/o timeout (Client.Timeout exceeded while awaiting headers)
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Liveness probe failed: Get "https://10.42.2.145:10250/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Liveness probe failed: HTTP probe failed with statuscode: 503
kube-system            74d         Warning   Unhealthy                           pod/traefik-5cbdcf97f4-bbnx7                                   Readiness probe failed: HTTP probe failed with statuscode: 404
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-prometheus-node-exporter-8dgb6              Liveness probe failed: Get "http://192.168.0.18:9100/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/coredns-ccb96694c-2mrz9                                    Liveness probe failed: Get "http://10.42.2.125:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/metrics-server-7bf7d58749-v5vbk                            Readiness probe failed: Get "https://10.42.2.124:10250/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/metrics-server-7bf7d58749-v5vbk                            Liveness probe failed: Get "https://10.42.2.124:10250/livez": context deadline exceeded
kube-system            74d         Warning   Unhealthy                           pod/kite-7cb79df49c-nth8h                                      Liveness probe failed: Get "http://10.42.2.140:8080/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Liveness probe failed: Get "http://10.42.2.134:8080/livez": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/kite-7cb79df49c-nth8h                                      Readiness probe failed: Get "http://10.42.2.140:8080/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/metrics-server-7bf7d58749-v5vbk                            Readiness probe failed: Get "https://10.42.2.124:10250/readyz": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
monitoring             74d         Normal    Killing                             pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Container kube-state-metrics failed liveness probe, will be restarted
kube-system            74d         Normal    Killing                             pod/kite-7cb79df49c-nth8h                                      Container kite failed liveness probe, will be restarted
monitoring             74d         Normal    Killing                             pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Container kube-prometheus-stack failed liveness probe, will be restarted
monitoring             74d         Warning   Unhealthy                           pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Liveness probe failed: Get "http://10.42.2.126:9093/-/healthy": dial tcp 10.42.2.126:9093: i/o timeout (Client.Timeout exceeded while awaiting headers)
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Readiness probe failed: Get "https://10.42.2.145:10250/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/traefik-5cbdcf97f4-bbnx7                                   Readiness probe failed: Get "http://10.42.2.129:9000/ping": dial tcp 10.42.2.129:9000: connect: connection refused (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/metrics-server-7bf7d58749-v5vbk                            Liveness probe failed: Get "https://10.42.2.124:10250/livez": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/coredns-ccb96694c-2mrz9                                    Readiness probe failed: Get "http://10.42.2.125:8181/ready": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
cyclops                74d         Warning   Unhealthy                           pod/cyclops-ctrl-8685b986c7-tmr5r                              Liveness probe failed: Get "http://10.42.2.147:8082/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Readiness probe failed: Get "https://10.42.2.145:10250/healthz": context deadline exceeded
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Readiness probe failed: Get "http://10.42.2.134:8081/readyz": EOF
argocd                 74d         Warning   Unhealthy                           pod/argocd-server-84477d5958-z5nwc                             Readiness probe failed: Get "http://10.42.2.144:8888/healthz": dial tcp 10.42.2.144:8888: connect: connection refused (Client.Timeout exceeded while awaiting headers)
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-677d8d6465-tktwr                            Readiness probe failed: Get "http://10.42.2.118:3000/api/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
monitoring             74d         Normal    Pulled                              pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Container image "registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0" already present on machine
kube-system            74d         Normal    Pulled                              pod/traefik-5cbdcf97f4-bbnx7                                   Container image "rancher/mirrored-library-traefik:2.11.20" already present on machine
monitoring             74d         Normal    Killing                             pod/monitorgrafana-prometheus-node-exporter-8dgb6              Container node-exporter failed liveness probe, will be restarted
argocd                 74d         Warning   Unhealthy                           pod/argocd-server-84477d5958-z5nwc                             Liveness probe failed: Get "http://10.42.2.144:8888/healthz?full=true": dial tcp 10.42.2.144:8888: connect: connection refused
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-prometheus-node-exporter-8dgb6              Readiness probe failed: Get "http://192.168.0.18:9100/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/traefik-5cbdcf97f4-bbnx7                                   Readiness probe failed: Get "http://10.42.2.129:9000/ping": dial tcp 10.42.2.129:9000: connect: connection refused
monitoring             74d         Normal    Pulled                              pod/monitorgrafana-prometheus-node-exporter-8dgb6              Container image "quay.io/prometheus/node-exporter:v1.9.1" already present on machine
pauhome                74d         Warning   FailedGetResourceMetric             horizontalpodautoscaler/web-hpa                                failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
pauhome                74d         Warning   FailedComputeMetricsReplicas        horizontalpodautoscaler/web-hpa                                invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
default                74d         Normal    CertificateExpirationOK             node/debianworker1                                             Node and Certificate Authority certificates managed by k3s are OK
default                74d         Warning   PossibleMemoryBackedVolumesOnDisk   node/debianworker1                                             The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
default                74d         Normal    NodeAllocatableEnforced             node/debianworker1                                             Updated Node Allocatable limit across pods
default                74d         Normal    Starting                            node/debianworker1                                             Starting kubelet.
default                74d         Warning   InvalidDiskCapacity                 node/debianworker1                                             invalid capacity 0 on image filesystem
argocd                 74d         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.24 192.168.0.25] -> [192.168.0.18 192.168.0.25]
kube-system            74d         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.24 192.168.0.25] -> [192.168.0.18 192.168.0.25]
default                74d         Normal    NodeHasSufficientPID                node/debianworker1                                             Node debianworker1 status is now: NodeHasSufficientPID
default                74d         Normal    NodeHasSufficientMemory             node/debianworker1                                             Node debianworker1 status is now: NodeHasSufficientMemory
default                74d         Normal    NodeHasNoDiskPressure               node/debianworker1                                             Node debianworker1 status is now: NodeHasNoDiskPressure
argocd                 74d         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.18 192.168.0.24 192.168.0.25]
kube-system            74d         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.18 192.168.0.24 192.168.0.25]
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-prometheus-node-exporter-8dgb6              Readiness probe failed: Get "http://192.168.0.18:9100/": dial tcp 192.168.0.18:9100: connect: connection refused
monitoring             74d         Normal    Created                             pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Created container: kube-state-metrics
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Readiness probe failed: Get "http://10.42.2.134:8081/readyz": dial tcp 10.42.2.134:8081: connect: connection refused
monitoring             74d         Normal    Started                             pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Started container kube-state-metrics
cyclops                74d         Warning   Unhealthy                           pod/cyclops-ctrl-8685b986c7-tmr5r                              Readiness probe failed: Get "http://10.42.2.147:8082/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kubernetes-dashboard   74d         Warning   Unhealthy                           pod/kubernetes-dashboard-8696f5f494-js7b8                      Liveness probe failed: Get "https://10.42.2.137:8443/": context deadline exceeded
argocd                 74d         Warning   Unhealthy                           pod/argocd-repo-server-95c9d69c6-2vxj4                         Liveness probe failed: Get "http://10.42.2.142:8084/healthz?full=true": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
argocd                 74d         Warning   Unhealthy                           pod/argocd-server-84477d5958-z5nwc                             Readiness probe failed: Get "http://10.42.2.144:8888/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
argocd                 74d         Warning   Unhealthy                           pod/argocd-repo-server-95c9d69c6-2vxj4                         Readiness probe failed: Get "http://10.42.2.142:8084/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kubernetes-dashboard   74d         Warning   Unhealthy                           pod/kubernetes-dashboard-8696f5f494-js7b8                      Liveness probe failed: Get "https://10.42.2.137:8443/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
cyclops                74d         Warning   FailedToUpdateEndpoint              endpoints/cyclops-ctrl                                         Failed to update endpoint cyclops/cyclops-ctrl: Put "https://127.0.0.1:6444/api/v1/namespaces/cyclops/endpoints/cyclops-ctrl": http2: client connection lost
argocd                 74d         Warning   Unhealthy                           pod/argocd-notifications-controller-b9986964f-llxlm            Liveness probe failed: dial tcp 10.42.2.130:9001: i/o timeout
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Readiness probe failed: Get "http://10.42.2.134:8081/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
cyclops                74d         Warning   FailedToUpdateEndpointSlices        service/cyclops-ctrl                                           Error updating Endpoint Slices for Service cyclops/cyclops-ctrl: failed to update cyclops-ctrl-phfph EndpointSlice for Service cyclops/cyclops-ctrl: Put "https://127.0.0.1:6444/apis/discovery.k8s.io/v1/namespaces/cyclops/endpointslices/cyclops-ctrl-phfph": http2: client connection lost
kubernetes-dashboard   74d         Warning   Unhealthy                           pod/kubernetes-dashboard-8696f5f494-js7b8                      Liveness probe failed: Get "https://10.42.2.137:8443/": dial tcp 10.42.2.137:8443: connect: connection refused
monitoring             74d         Warning   Unhealthy                           pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Readiness probe failed: Get "http://10.42.2.126:9093/-/ready": dial tcp 10.42.2.126:9093: i/o timeout (Client.Timeout exceeded while awaiting headers)
default                74d         Normal    NodeNotReady                        node/debianmaster                                              Node debianmaster status is now: NodeNotReady
default                74d         Normal    NodeNotReady                        node/debianworker1                                             Node debianworker1 status is now: NodeNotReady
monitoring             74d         Warning   Unhealthy                           pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Readiness probe failed: Get "http://10.42.2.126:9093/-/ready": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
monitoring             74d         Warning   Unhealthy                           pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Liveness probe failed: Get "http://10.42.2.126:9093/-/healthy": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
default                74d         Normal    NodeHasSufficientPID                node/debianmaster                                              Node debianmaster status is now: NodeHasSufficientPID
monitoring             74d         Warning   NodeNotReady                        pod/monitorgrafana-prometheus-node-exporter-2hhbs              Node is not ready
default                74d         Normal    NodeHasSufficientMemory             node/debianmaster                                              Node debianmaster status is now: NodeHasSufficientMemory
default                74d         Normal    NodeHasNoDiskPressure               node/debianmaster                                              Node debianmaster status is now: NodeHasNoDiskPressure
argocd                 74d         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.24 192.168.0.25] -> [192.168.0.18 192.168.0.24 192.168.0.25]
cyclops                74d         Warning   FailedToUpdateEndpoint              endpoints/cyclops-ctrl                                         Failed to update endpoint cyclops/cyclops-ctrl: Operation cannot be fulfilled on endpoints "cyclops-ctrl": the object has been modified; please apply your changes to the latest version and try again
cyclops                74d         Warning   NodeNotReady                        pod/cyclops-ui-97ffc847c-qz262                                 Node is not ready
kube-system            74d         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.24 192.168.0.25] -> [192.168.0.18 192.168.0.24 192.168.0.25]
kube-system            74d         Warning   NodeNotReady                        pod/svclb-argocd-server-28f86d56-qr66h                         Node is not ready
kube-system            74d         Warning   NodeNotReady                        pod/svclb-traefik-cd2bf767-68vkn                               Node is not ready
monitoring             74d         Warning   NodeNotReady                        pod/loki-promtail-9g6nk                                        Node is not ready
kube-system            74d         Warning   NodeNotReady                        pod/svclb-argocd-server-28f86d56-4jt9k                         Node is not ready
pauhome                74d         Warning   NodeNotReady                        pod/web-595c7c58b5-vpjgw                                       Node is not ready
default                74d         Warning   NodeNotReady                        pod/testnet                                                    Node is not ready
kubernetes-dashboard   74d         Warning   NodeNotReady                        pod/kubernetes-dashboard-8696f5f494-js7b8                      Node is not ready
argocd                 74d         Normal    Killing                             pod/argocd-notifications-controller-b9986964f-llxlm            Container argocd-notifications-controller failed liveness probe, will be restarted
pauhome                74d         Warning   NodeNotReady                        pod/web-595c7c58b5-n4kjr                                       Node is not ready
pauhome                74d         Warning   NodeNotReady                        pod/web-595c7c58b5-zrmcp                                       Node is not ready
argocd                 74d         Normal    Pulling                             pod/argocd-notifications-controller-b9986964f-llxlm            Pulling image "quay.io/argoproj/argocd:v3.1.1"
argocd                 74d         Normal    Pulled                              pod/argocd-notifications-controller-b9986964f-llxlm            Successfully pulled image "quay.io/argoproj/argocd:v3.1.1" in 2.075s (2.075s including waiting). Image size: 191031950 bytes.
argocd                 74d         Normal    Started                             pod/argocd-notifications-controller-b9986964f-llxlm            Started container argocd-notifications-controller
argocd                 74d         Normal    Created                             pod/argocd-notifications-controller-b9986964f-llxlm            Created container: argocd-notifications-controller
default                74d         Warning   NodeNotReady                        pod/hello-world-795988dcf6-gjhr9                               Node is not ready
kube-system            74d         Warning   NodeNotReady                        pod/coredns-ccb96694c-2mrz9                                    Node is not ready
argocd                 74d         Warning   NodeNotReady                        pod/argocd-dex-server-55bdf84957-lbcgk                         Node is not ready
monitoring             74d         Warning   Unhealthy                           pod/loki-promtail-5dr6s                                        Readiness probe failed: Get "http://10.42.2.139:3101/ready": dial tcp 10.42.2.139:3101: i/o timeout (Client.Timeout exceeded while awaiting headers)
argocd                 74d         Warning   FailedToUpdateEndpoint              endpoints/argocd-notifications-controller-metrics              Failed to update endpoint argocd/argocd-notifications-controller-metrics: Put "https://127.0.0.1:6444/api/v1/namespaces/argocd/endpoints/argocd-notifications-controller-metrics": http2: client connection lost
argocd                 74d         Warning   FailedToUpdateEndpointSlices        service/argocd-notifications-controller-metrics                Error updating Endpoint Slices for Service argocd/argocd-notifications-controller-metrics: failed to update argocd-notifications-controller-metrics-t92rr EndpointSlice for Service argocd/argocd-notifications-controller-metrics: Put "https://127.0.0.1:6444/apis/discovery.k8s.io/v1/namespaces/argocd/endpointslices/argocd-notifications-controller-metrics-t92rr": http2: client connection lost
kubernetes-dashboard   74d         Warning   Unhealthy                           pod/dashboard-metrics-scraper-6b96ff7878-4r6ks                 Liveness probe failed: Get "http://10.42.2.119:8000/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/headlamp-666485dd85-hpxln                                  Liveness probe failed: Get "http://10.42.2.135:4466/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
kube-system            74d         Warning   Unhealthy                           pod/headlamp-666485dd85-hpxln                                  Readiness probe failed: Get "http://10.42.2.135:4466/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
default                74d         Normal    NodeNotReady                        node/debianmaster                                              Node debianmaster status is now: NodeNotReady
default                74d         Warning   ContainerGCFailed                   node/debianmaster                                              rpc error: code = DeadlineExceeded desc = context deadline exceeded
pauhome                74d         Warning   NodeNotReady                        pod/web-595c7c58b5-mcb6x                                       Node is not ready
kubernetes-dashboard   74d         Warning   NodeNotReady                        pod/dashboard-metrics-scraper-6b96ff7878-4r6ks                 Node is not ready
argocd                 74d         Warning   FailedToUpdateEndpoint              endpoints/argocd-notifications-controller-metrics              Failed to update endpoint argocd/argocd-notifications-controller-metrics: Operation cannot be fulfilled on endpoints "argocd-notifications-controller-metrics": the object has been modified; please apply your changes to the latest version and try again
pauhome                74d         Warning   FailedComputeMetricsReplicas        horizontalpodautoscaler/web-hpa                                invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
argocd                 74d         Warning   NodeNotReady                        pod/argocd-redis-65b9d6c547-xqlsp                              Node is not ready
argocd                 74d         Warning   NodeNotReady                        pod/argocd-applicationset-controller-75c59fc666-bkzbt          Node is not ready
pauhome                74d         Warning   FailedGetResourceMetric             horizontalpodautoscaler/web-hpa                                failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
kube-system            74d         Normal    Killing                             pod/headlamp-666485dd85-hpxln                                  Container headlamp failed liveness probe, will be restarted
kube-system            74d         Warning   NodeNotReady                        pod/svclb-traefik-cd2bf767-2k6jc                               Node is not ready
kube-system            74d         Normal    Pulling                             pod/headlamp-666485dd85-hpxln                                  Pulling image "ghcr.io/headlamp-k8s/headlamp:latest"
argocd                 74d         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.24 192.168.0.25] -> [192.168.0.25]
kube-system            74d         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.24 192.168.0.25] -> [192.168.0.25]
kube-system            74d         Normal    Pulled                              pod/headlamp-666485dd85-hpxln                                  Successfully pulled image "ghcr.io/headlamp-k8s/headlamp:latest" in 2.461s (2.469s including waiting). Image size: 98075925 bytes.
kube-system            74d         Normal    ApplyJob                            helmchart/traefik-crd                                          Applying HelmChart using Job kube-system/helm-install-traefik-crd
kube-system            74d         Normal    ApplyJob                            helmchart/traefik                                              Applying HelmChart using Job kube-system/helm-install-traefik
kube-system            74d         Normal    Created                             pod/headlamp-666485dd85-hpxln                                  Created container: headlamp
kube-system            74d         Normal    Started                             pod/headlamp-666485dd85-hpxln                                  Started container headlamp
default                74d         Normal    NodeReady                           node/debianmaster                                              Node debianmaster status is now: NodeReady
monitoring             74d         Warning   NodeNotReady                        pod/loki-promtail-5dr6s                                        Node is not ready
kube-system            74d         Warning   Unhealthy                           pod/headlamp-666485dd85-hpxln                                  Readiness probe failed: Get "http://10.42.2.135:4466/": dial tcp 10.42.2.135:4466: connect: connection refused
monitoring             74d         Warning   NodeNotReady                        pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Node is not ready
default                74d         Warning   NodeNotReady                        pod/nginx-676b6c5bbc-kcg8n                                     Node is not ready
kube-system            74d         Warning   NodeNotReady                        pod/headlamp-666485dd85-hpxln                                  Node is not ready
monitoring             74d         Warning   NodeNotReady                        pod/monitorgrafana-prometheus-node-exporter-8dgb6              Node is not ready
kube-system            74d         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.25] -> [192.168.0.18 192.168.0.25]
kube-system            74d         Warning   NodeNotReady                        pod/kite-7cb79df49c-nth8h                                      Node is not ready
kube-system            74d         Warning   NodeNotReady                        pod/traefik-5cbdcf97f4-bbnx7                                   Node is not ready
argocd                 74d         Warning   NodeNotReady                        pod/argocd-notifications-controller-b9986964f-llxlm            Node is not ready
kube-system            74d         Warning   NodeNotReady                        pod/local-path-provisioner-5b5f758bcf-7hq92                    Node is not ready
pauhome                74d         Warning   NodeNotReady                        pod/web-595c7c58b5-9vs9d                                       Node is not ready
argocd                 74d         Warning   NodeNotReady                        pod/argocd-repo-server-95c9d69c6-2vxj4                         Node is not ready
cyclops                74d         Warning   NodeNotReady                        pod/cyclops-ctrl-8685b986c7-tmr5r                              Node is not ready
argocd                 74d         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.25] -> [192.168.0.18 192.168.0.25]
monitoring             74d         Warning   Unhealthy                           pod/monitorgrafana-677d8d6465-tktwr                            Readiness probe failed: Get "http://10.42.2.118:3000/api/health": dial tcp 10.42.2.118:3000: connect: connection refused
kube-system            74d         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.18 192.168.0.25]
monitoring             74d         Normal    TaintManagerEviction                pod/loki-0                                                     Marking for deletion Pod monitoring/loki-0
argocd                 74d         Normal    TaintManagerEviction                pod/argocd-application-controller-0                            Marking for deletion Pod argocd/argocd-application-controller-0
monitoring             74d         Normal    TaintManagerEviction                pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Marking for deletion Pod monitoring/prometheus-monitorgrafana-kube-promet-prometheus-0
kubernetes-dashboard   74d         Normal    Killing                             pod/kubernetes-dashboard-8696f5f494-js7b8                      Container kubernetes-dashboard failed liveness probe, will be restarted
kubernetes-dashboard   74d         Normal    Pulling                             pod/kubernetes-dashboard-8696f5f494-js7b8                      Pulling image "kubernetesui/dashboard:v2.7.0"
kubernetes-dashboard   74d         Normal    Pulled                              pod/kubernetes-dashboard-8696f5f494-js7b8                      Successfully pulled image "kubernetesui/dashboard:v2.7.0" in 945ms (950ms including waiting). Image size: 75788960 bytes.
kubernetes-dashboard   74d         Normal    Started                             pod/kubernetes-dashboard-8696f5f494-js7b8                      Started container kubernetes-dashboard
kubernetes-dashboard   74d         Normal    Created                             pod/kubernetes-dashboard-8696f5f494-js7b8                      Created container: kubernetes-dashboard
monitoring             74d         Warning   Unhealthy                           pod/loki-promtail-5dr6s                                        Readiness probe failed: Get "http://10.42.2.139:3101/ready": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
default                74d         Normal    TaintManagerEviction                pod/testnet                                                    Marking for deletion Pod default/testnet
argocd                 74d         Normal    Pulling                             pod/argocd-server-84477d5958-z5nwc                             Pulling image "quay.io/argoproj/argocd:v3.1.1"
argocd                 74d         Warning   BackOff                             pod/argocd-server-84477d5958-z5nwc                             Back-off restarting failed container argocd-server in pod argocd-server-84477d5958-z5nwc_argocd(17f73213-6a33-4711-a57e-16b3436732fc)
monitoring             74d         Warning   BackOff                             pod/monitorgrafana-677d8d6465-tktwr                            Back-off restarting failed container grafana in pod monitorgrafana-677d8d6465-tktwr_monitoring(70ce476e-785e-4363-a2e4-69a553b0c7b7)
argocd                 74d         Warning   Unhealthy                           pod/argocd-server-84477d5958-z5nwc                             Readiness probe failed: Get "http://10.42.2.144:8888/healthz": dial tcp 10.42.2.144:8888: connect: connection refused
default                34m         Warning   PossibleMemoryBackedVolumesOnDisk   node/debianmaster                                              The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
default                34m         Normal    Starting                            node/debianmaster                                              Starting kubelet.
argocd                 34m         Warning   FailedMount                         pod/argocd-server-84477d5958-z5nwc                             MountVolume.SetUp failed for volume "argocd-dex-server-tls" : object "argocd"/"argocd-dex-server-tls" not registered
default                34m         Normal    NodeAllocatableEnforced             node/debianmaster                                              Updated Node Allocatable limit across pods
default                34m         Normal    NodeHasSufficientMemory             node/debianmaster                                              Node debianmaster status is now: NodeHasSufficientMemory
default                34m         Normal    NodeHasNoDiskPressure               node/debianmaster                                              Node debianmaster status is now: NodeHasNoDiskPressure
default                34m         Normal    NodeHasSufficientPID                node/debianmaster                                              Node debianmaster status is now: NodeHasSufficientPID
kube-system            34m         Normal    SandboxChanged                      pod/kite-7cb79df49c-nth8h                                      Pod sandbox changed, it will be killed and re-created.
default                34m         Warning   Rebooted                            node/debianmaster                                              Node debianmaster has been rebooted, boot id: 69a20a1a-cbb0-4e05-ab10-d2f48ce59b2d
argocd                 34m         Warning   FailedMount                         pod/argocd-server-84477d5958-z5nwc                             MountVolume.SetUp failed for volume "argocd-cmd-params-cm" : object "argocd"/"argocd-cmd-params-cm" not registered
monitoring             34m         Normal    SandboxChanged                      pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    Pulled                              pod/monitorgrafana-prometheus-node-exporter-8dgb6              Container image "quay.io/prometheus/node-exporter:v1.9.1" already present on machine
argocd                 34m         Warning   FailedMount                         pod/argocd-dex-server-55bdf84957-lbcgk                         MountVolume.SetUp failed for volume "argocd-dex-server-tls" : object "argocd"/"argocd-dex-server-tls" not registered
monitoring             34m         Warning   FailedMount                         pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          MountVolume.SetUp failed for volume "tls-secret" : object "monitoring"/"monitorgrafana-kube-promet-admission" not registered
monitoring             34m         Normal    SandboxChanged                      pod/monitorgrafana-prometheus-node-exporter-8dgb6              Pod sandbox changed, it will be killed and re-created.
argocd                 34m         Warning   FailedMount                         pod/argocd-notifications-controller-b9986964f-llxlm            MountVolume.SetUp failed for volume "kube-api-access-grvq2" : object "argocd"/"kube-root-ca.crt" not registered
kube-system            34m         Normal    Created                             pod/kite-7cb79df49c-nth8h                                      Created container: kite
kube-system            34m         Normal    SandboxChanged                      pod/metrics-server-7bf7d58749-v5vbk                            Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    SandboxChanged                      pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Warning   FailedMount                         pod/monitorgrafana-677d8d6465-tktwr                            MountVolume.SetUp failed for volume "sc-dashboard-provider" : object "monitoring"/"monitorgrafana-config-dashboards" not registered
argocd                 34m         Normal    SandboxChanged                      pod/argocd-dex-server-55bdf84957-lbcgk                         Pod sandbox changed, it will be killed and re-created.
kube-system            34m         Normal    Started                             pod/kite-7cb79df49c-nth8h                                      Started container kite
monitoring             34m         Warning   FailedMount                         pod/monitorgrafana-677d8d6465-tktwr                            MountVolume.SetUp failed for volume "config" : object "monitoring"/"monitorgrafana" not registered
monitoring             34m         Normal    Created                             pod/monitorgrafana-prometheus-node-exporter-8dgb6              Created container: node-exporter
monitoring             34m         Normal    Started                             pod/monitorgrafana-prometheus-node-exporter-8dgb6              Started container node-exporter
kube-system            34m         Normal    Pulled                              pod/kite-7cb79df49c-nth8h                                      Container image "ghcr.io/zxh326/kite:latest" already present on machine
pauhome                34m         Warning   FailedMount                         pod/web-595c7c58b5-9vs9d                                       MountVolume.SetUp failed for volume "kube-api-access-42rlr" : object "pauhome"/"kube-root-ca.crt" not registered
pauhome                34m         Warning   FailedMount                         pod/web-595c7c58b5-mcb6x                                       MountVolume.SetUp failed for volume "kube-api-access-tkx2j" : object "pauhome"/"kube-root-ca.crt" not registered
cyclops                34m         Warning   FailedMount                         pod/cyclops-ctrl-8685b986c7-tmr5r                              MountVolume.SetUp failed for volume "kube-api-access-9nt5h" : object "cyclops"/"kube-root-ca.crt" not registered
monitoring             34m         Normal    Pulled                              pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Container image "quay.io/prometheus-operator/prometheus-operator:v0.85.0" already present on machine
kubernetes-dashboard   34m         Warning   FailedMount                         pod/kubernetes-dashboard-8696f5f494-js7b8                      MountVolume.SetUp failed for volume "kube-api-access-2s669" : object "kubernetes-dashboard"/"kube-root-ca.crt" not registered
kube-system            34m         Normal    SandboxChanged                      pod/headlamp-666485dd85-hpxln                                  Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    Pulled                              pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0" already present on machine
kubernetes-dashboard   34m         Warning   FailedMount                         pod/dashboard-metrics-scraper-6b96ff7878-4r6ks                 MountVolume.SetUp failed for volume "kube-api-access-t85ff" : object "kubernetes-dashboard"/"kube-root-ca.crt" not registered
pauhome                34m         Warning   FailedMount                         pod/web-595c7c58b5-n4kjr                                       MountVolume.SetUp failed for volume "kube-api-access-mhpb9" : object "pauhome"/"kube-root-ca.crt" not registered
monitoring             34m         Normal    Created                             pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Created container: init-config-reloader
monitoring             34m         Normal    Started                             pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Started container init-config-reloader
pauhome                34m         Warning   FailedMount                         pod/web-595c7c58b5-vpjgw                                       MountVolume.SetUp failed for volume "kube-api-access-k9qvf" : object "pauhome"/"kube-root-ca.crt" not registered
cyclops                34m         Warning   FailedMount                         pod/cyclops-ui-97ffc847c-qz262                                 MountVolume.SetUp failed for volume "kube-api-access-x9t4s" : object "cyclops"/"kube-root-ca.crt" not registered
kube-system            34m         Normal    Pulled                              pod/metrics-server-7bf7d58749-v5vbk                            Container image "rancher/mirrored-metrics-server:v0.7.2" already present on machine
pauhome                34m         Warning   FailedMount                         pod/web-595c7c58b5-zrmcp                                       MountVolume.SetUp failed for volume "kube-api-access-6kp2r" : object "pauhome"/"kube-root-ca.crt" not registered
kubernetes-dashboard   34m         Warning   FailedMount                         pod/kubernetes-dashboard-8696f5f494-js7b8                      MountVolume.SetUp failed for volume "kubernetes-dashboard-certs" : object "kubernetes-dashboard"/"kubernetes-dashboard-certs" not registered
argocd                 34m         Warning   FailedMount                         pod/argocd-repo-server-95c9d69c6-2vxj4                         MountVolume.SetUp failed for volume "ssh-known-hosts" : object "argocd"/"argocd-ssh-known-hosts-cm" not registered
kube-system            34m         Normal    Created                             pod/metrics-server-7bf7d58749-v5vbk                            Created container: metrics-server
argocd                 34m         Warning   FailedMount                         pod/argocd-server-84477d5958-z5nwc                             MountVolume.SetUp failed for volume "argocd-repo-server-tls" : object "argocd"/"argocd-repo-server-tls" not registered
kube-system            34m         Normal    Pulling                             pod/headlamp-666485dd85-hpxln                                  Pulling image "ghcr.io/headlamp-k8s/headlamp:latest"
argocd                 34m         Warning   FailedMount                         pod/argocd-applicationset-controller-75c59fc666-bkzbt          MountVolume.SetUp failed for volume "ssh-known-hosts" : object "argocd"/"argocd-ssh-known-hosts-cm" not registered
argocd                 34m         Warning   FailedMount                         pod/argocd-notifications-controller-b9986964f-llxlm            MountVolume.SetUp failed for volume "argocd-repo-server-tls" : object "argocd"/"argocd-repo-server-tls" not registered
argocd                 34m         Warning   FailedMount                         pod/argocd-notifications-controller-b9986964f-llxlm            MountVolume.SetUp failed for volume "tls-certs" : object "argocd"/"argocd-tls-certs-cm" not registered
kube-system            34m         Normal    Started                             pod/metrics-server-7bf7d58749-v5vbk                            Started container metrics-server
monitoring             34m         Normal    Started                             pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Started container kube-prometheus-stack
monitoring             34m         Normal    Created                             pod/monitorgrafana-kube-promet-operator-7fffcb5-d4rhz          Created container: kube-prometheus-stack
argocd                 34m         Normal    Created                             pod/argocd-dex-server-55bdf84957-lbcgk                         Created container: copyutil
argocd                 34m         Normal    Pulled                              pod/argocd-dex-server-55bdf84957-lbcgk                         Successfully pulled image "quay.io/argoproj/argocd:v3.1.1" in 652ms (652ms including waiting). Image size: 191031950 bytes.
argocd                 34m         Warning   FailedMount                         pod/argocd-applicationset-controller-75c59fc666-bkzbt          MountVolume.SetUp failed for volume "argocd-repo-server-tls" : object "argocd"/"argocd-repo-server-tls" not registered
kube-system            34m         Normal    SandboxChanged                      pod/svclb-argocd-server-28f86d56-4jt9k                         Pod sandbox changed, it will be killed and re-created.
argocd                 34m         Warning   FailedMount                         pod/argocd-repo-server-95c9d69c6-2vxj4                         MountVolume.SetUp failed for volume "tls-certs" : object "argocd"/"argocd-tls-certs-cm" not registered
argocd                 34m         Warning   FailedMount                         pod/argocd-server-84477d5958-z5nwc                             MountVolume.SetUp failed for volume "tls-certs" : object "argocd"/"argocd-tls-certs-cm" not registered
monitoring             34m         Warning   FailedMount                         pod/loki-promtail-5dr6s                                        MountVolume.SetUp failed for volume "config" : object "monitoring"/"loki-promtail" not registered
argocd                 34m         Warning   FailedMount                         pod/argocd-server-84477d5958-z5nwc                             MountVolume.SetUp failed for volume "ssh-known-hosts" : object "argocd"/"argocd-ssh-known-hosts-cm" not registered
argocd                 34m         Warning   FailedMount                         pod/argocd-repo-server-95c9d69c6-2vxj4                         MountVolume.SetUp failed for volume "argocd-repo-server-tls" : object "argocd"/"argocd-repo-server-tls" not registered
argocd                 34m         Warning   FailedMount                         pod/argocd-applicationset-controller-75c59fc666-bkzbt          MountVolume.SetUp failed for volume "tls-certs" : object "argocd"/"argocd-tls-certs-cm" not registered
argocd                 34m         Normal    Pulling                             pod/argocd-dex-server-55bdf84957-lbcgk                         Pulling image "quay.io/argoproj/argocd:v3.1.1"
monitoring             34m         Normal    SandboxChanged                      pod/monitorgrafana-677d8d6465-tktwr                            Pod sandbox changed, it will be killed and re-created.
kubernetes-dashboard   34m         Normal    SandboxChanged                      pod/dashboard-metrics-scraper-6b96ff7878-4r6ks                 Pod sandbox changed, it will be killed and re-created.
argocd                 34m         Normal    SandboxChanged                      pod/argocd-redis-65b9d6c547-xqlsp                              Pod sandbox changed, it will be killed and re-created.
argocd                 34m         Normal    Started                             pod/argocd-dex-server-55bdf84957-lbcgk                         Started container copyutil
pauhome                34m         Normal    SandboxChanged                      pod/web-595c7c58b5-vpjgw                                       Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    SandboxChanged                      pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Pod sandbox changed, it will be killed and re-created.
pauhome                34m         Normal    SandboxChanged                      pod/web-595c7c58b5-9vs9d                                       Pod sandbox changed, it will be killed and re-created.
cyclops                34m         Normal    SandboxChanged                      pod/cyclops-ctrl-8685b986c7-tmr5r                              Pod sandbox changed, it will be killed and re-created.
kube-system            34m         Normal    Pulled                              pod/svclb-argocd-server-28f86d56-4jt9k                         Container image "rancher/klipper-lb:v0.4.10" already present on machine
kube-system            34m         Normal    SandboxChanged                      pod/svclb-traefik-cd2bf767-2k6jc                               Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    Pulled                              pod/monitorgrafana-677d8d6465-tktwr                            Container image "quay.io/kiwigrid/k8s-sidecar:1.30.3" already present on machine
kube-system            34m         Normal    Started                             pod/svclb-argocd-server-28f86d56-4jt9k                         Started container lb-tcp-8082
kube-system            34m         Normal    Created                             pod/svclb-argocd-server-28f86d56-4jt9k                         Created container: lb-tcp-8082
monitoring             34m         Normal    Pulled                              pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Container image "registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0" already present on machine
monitoring             34m         Normal    Created                             pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Created container: kube-state-metrics
cyclops                34m         Normal    Pulled                              pod/cyclops-ctrl-8685b986c7-tmr5r                              Container image "cyclopsui/cyclops-ctrl:v0.21.1" already present on machine
cyclops                34m         Normal    Created                             pod/cyclops-ctrl-8685b986c7-tmr5r                              Created container: cyclops-ctrl
monitoring             34m         Normal    Started                             pod/monitorgrafana-kube-state-metrics-995596c59-b56pm          Started container kube-state-metrics
cyclops                34m         Normal    SandboxChanged                      pod/cyclops-ui-97ffc847c-qz262                                 Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    Created                             pod/monitorgrafana-677d8d6465-tktwr                            Created container: grafana-sc-dashboard
pauhome                34m         Normal    Pulled                              pod/web-595c7c58b5-9vs9d                                       Container image "nginx:stable" already present on machine
kubernetes-dashboard   34m         Normal    Pulled                              pod/dashboard-metrics-scraper-6b96ff7878-4r6ks                 Container image "kubernetesui/metrics-scraper:v1.0.8" already present on machine
kube-system            34m         Warning   FailedMount                         pod/coredns-ccb96694c-2mrz9                                    MountVolume.SetUp failed for volume "custom-config-volume" : object "kube-system"/"coredns-custom" not registered
kube-system            34m         Warning   FailedMount                         pod/coredns-ccb96694c-2mrz9                                    MountVolume.SetUp failed for volume "config-volume" : object "kube-system"/"coredns" not registered
kube-system            34m         Warning   FailedMount                         pod/local-path-provisioner-5b5f758bcf-7hq92                    MountVolume.SetUp failed for volume "config-volume" : object "kube-system"/"local-path-config" not registered
cyclops                34m         Normal    Started                             pod/cyclops-ctrl-8685b986c7-tmr5r                              Started container cyclops-ctrl
pauhome                34m         Normal    Created                             pod/web-595c7c58b5-9vs9d                                       Created container: web
argocd                 34m         Normal    SandboxChanged                      pod/argocd-server-84477d5958-z5nwc                             Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    Started                             pod/monitorgrafana-677d8d6465-tktwr                            Started container grafana-sc-dashboard
argocd                 34m         Warning   FailedMount                         pod/argocd-repo-server-95c9d69c6-2vxj4                         MountVolume.SetUp failed for volume "gpg-keys" : object "argocd"/"argocd-gpg-keys-cm" not registered
pauhome                34m         Normal    Pulled                              pod/web-595c7c58b5-vpjgw                                       Container image "nginx:stable" already present on machine
monitoring             34m         Normal    Pulled                              pod/monitorgrafana-677d8d6465-tktwr                            Container image "quay.io/kiwigrid/k8s-sidecar:1.30.3" already present on machine
kubernetes-dashboard   34m         Normal    Created                             pod/dashboard-metrics-scraper-6b96ff7878-4r6ks                 Created container: dashboard-metrics-scraper
argocd                 34m         Warning   FailedMount                         pod/argocd-applicationset-controller-75c59fc666-bkzbt          MountVolume.SetUp failed for volume "gpg-keys" : object "argocd"/"argocd-gpg-keys-cm" not registered
monitoring             34m         Normal    SandboxChanged                      pod/loki-promtail-5dr6s                                        Pod sandbox changed, it will be killed and re-created.
kubernetes-dashboard   34m         Normal    SandboxChanged                      pod/kubernetes-dashboard-8696f5f494-js7b8                      Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    Created                             pod/monitorgrafana-677d8d6465-tktwr                            Created container: grafana-sc-datasources
pauhome                34m         Normal    Created                             pod/web-595c7c58b5-vpjgw                                       Created container: web
kubernetes-dashboard   34m         Normal    Started                             pod/dashboard-metrics-scraper-6b96ff7878-4r6ks                 Started container dashboard-metrics-scraper
pauhome                34m         Normal    Started                             pod/web-595c7c58b5-9vs9d                                       Started container web
argocd                 34m         Normal    Pulled                              pod/argocd-redis-65b9d6c547-xqlsp                              Container image "quay.io/argoproj/argocd:v3.1.1" already present on machine
argocd                 34m         Normal    SandboxChanged                      pod/argocd-notifications-controller-b9986964f-llxlm            Pod sandbox changed, it will be killed and re-created.
default                34m         Warning   FailedMount                         pod/hello-world-795988dcf6-gjhr9                               MountVolume.SetUp failed for volume "kube-api-access-2psn5" : object "default"/"kube-root-ca.crt" not registered
default                34m         Warning   FailedMount                         pod/nginx-676b6c5bbc-kcg8n                                     MountVolume.SetUp failed for volume "kube-api-access-2b8qk" : object "default"/"kube-root-ca.crt" not registered
argocd                 34m         Normal    Created                             pod/argocd-redis-65b9d6c547-xqlsp                              Created container: secret-init
pauhome                34m         Normal    SandboxChanged                      pod/web-595c7c58b5-zrmcp                                       Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    Started                             pod/monitorgrafana-677d8d6465-tktwr                            Started container grafana-sc-datasources
pauhome                34m         Normal    Started                             pod/web-595c7c58b5-vpjgw                                       Started container web
kube-system            34m         Normal    Pulled                              pod/svclb-traefik-cd2bf767-2k6jc                               Container image "rancher/klipper-lb:v0.4.10" already present on machine
argocd                 34m         Normal    Started                             pod/argocd-redis-65b9d6c547-xqlsp                              Started container secret-init
kubernetes-dashboard   34m         Normal    Pulling                             pod/kubernetes-dashboard-8696f5f494-js7b8                      Pulling image "kubernetesui/dashboard:v2.7.0"
monitoring             34m         Normal    Pulled                              pod/loki-promtail-5dr6s                                        Container image "docker.io/grafana/promtail:2.9.3" already present on machine
argocd                 34m         Normal    Pulling                             pod/argocd-server-84477d5958-z5nwc                             Pulling image "quay.io/argoproj/argocd:v3.1.1"
kube-system            34m         Normal    Created                             pod/svclb-traefik-cd2bf767-2k6jc                               Created container: lb-tcp-80
argocd                 34m         Normal    Pulling                             pod/argocd-notifications-controller-b9986964f-llxlm            Pulling image "quay.io/argoproj/argocd:v3.1.1"
kube-system            34m         Normal    Started                             pod/svclb-traefik-cd2bf767-2k6jc                               Started container lb-tcp-80
kube-system            34m         Normal    Pulled                              pod/svclb-traefik-cd2bf767-2k6jc                               Container image "rancher/klipper-lb:v0.4.10" already present on machine
monitoring             34m         Normal    Created                             pod/loki-promtail-5dr6s                                        Created container: promtail
monitoring             34m         Normal    Started                             pod/monitorgrafana-677d8d6465-tktwr                            Started container grafana
kube-system            34m         Normal    Created                             pod/svclb-traefik-cd2bf767-2k6jc                               Created container: lb-tcp-443
kubernetes-dashboard   34m         Normal    Pulled                              pod/kubernetes-dashboard-8696f5f494-js7b8                      Successfully pulled image "kubernetesui/dashboard:v2.7.0" in 2.47s (2.47s including waiting). Image size: 75788960 bytes.
argocd                 34m         Normal    Pulled                              pod/argocd-server-84477d5958-z5nwc                             Successfully pulled image "quay.io/argoproj/argocd:v3.1.1" in 2.279s (2.279s including waiting). Image size: 191031950 bytes.
monitoring             34m         Normal    Started                             pod/loki-promtail-5dr6s                                        Started container promtail
argocd                 34m         Normal    Pulled                              pod/argocd-notifications-controller-b9986964f-llxlm            Successfully pulled image "quay.io/argoproj/argocd:v3.1.1" in 2.111s (2.112s including waiting). Image size: 191031950 bytes.
pauhome                34m         Normal    Created                             pod/web-595c7c58b5-zrmcp                                       Created container: web
kube-system            34m         Normal    Started                             pod/svclb-traefik-cd2bf767-2k6jc                               Started container lb-tcp-443
argocd                 34m         Normal    Created                             pod/argocd-notifications-controller-b9986964f-llxlm            Created container: argocd-notifications-controller
pauhome                34m         Normal    Pulled                              pod/web-595c7c58b5-zrmcp                                       Container image "nginx:stable" already present on machine
argocd                 34m         Normal    Created                             pod/argocd-server-84477d5958-z5nwc                             Created container: argocd-server
kubernetes-dashboard   34m         Normal    Created                             pod/kubernetes-dashboard-8696f5f494-js7b8                      Created container: kubernetes-dashboard
pauhome                34m         Normal    Started                             pod/web-595c7c58b5-zrmcp                                       Started container web
argocd                 34m         Normal    Started                             pod/argocd-notifications-controller-b9986964f-llxlm            Started container argocd-notifications-controller
argocd                 34m         Normal    Started                             pod/argocd-server-84477d5958-z5nwc                             Started container argocd-server
kubernetes-dashboard   34m         Normal    Started                             pod/kubernetes-dashboard-8696f5f494-js7b8                      Started container kubernetes-dashboard
pauhome                34m         Normal    SandboxChanged                      pod/web-595c7c58b5-mcb6x                                       Pod sandbox changed, it will be killed and re-created.
argocd                 34m         Normal    SandboxChanged                      pod/argocd-applicationset-controller-75c59fc666-bkzbt          Pod sandbox changed, it will be killed and re-created.
kube-system            34m         Normal    SandboxChanged                      pod/local-path-provisioner-5b5f758bcf-7hq92                    Pod sandbox changed, it will be killed and re-created.
default                34m         Warning   NodePasswordValidationFailed        node/debianmaster                                              Deferred node password secret validation failed: secrets "debianmaster.node-password.k3s" already exists
kube-system            34m         Normal    SandboxChanged                      pod/coredns-ccb96694c-2mrz9                                    Pod sandbox changed, it will be killed and re-created.
kube-system            34m         Normal    SandboxChanged                      pod/traefik-5cbdcf97f4-bbnx7                                   Pod sandbox changed, it will be killed and re-created.
default                34m         Normal    SandboxChanged                      pod/nginx-676b6c5bbc-kcg8n                                     Pod sandbox changed, it will be killed and re-created.
pauhome                34m         Normal    SandboxChanged                      pod/web-595c7c58b5-n4kjr                                       Pod sandbox changed, it will be killed and re-created.
default                34m         Normal    SandboxChanged                      pod/hello-world-795988dcf6-gjhr9                               Pod sandbox changed, it will be killed and re-created.
kube-system            34m         Normal    ApplyingManifest                    addon/ccm                                                      Applying manifest at "/var/lib/rancher/k3s/server/manifests/ccm.yaml"
kube-system            34m         Normal    Pulled                              pod/coredns-ccb96694c-2mrz9                                    Container image "rancher/mirrored-coredns-coredns:1.12.0" already present on machine
argocd                 34m         Normal    SandboxChanged                      pod/argocd-repo-server-95c9d69c6-2vxj4                         Pod sandbox changed, it will be killed and re-created.
kube-system            34m         Normal    Created                             pod/coredns-ccb96694c-2mrz9                                    Created container: coredns
kube-system            34m         Normal    Pulled                              pod/traefik-5cbdcf97f4-bbnx7                                   Container image "rancher/mirrored-library-traefik:2.11.20" already present on machine
default                34m         Normal    NodePasswordValidationComplete      node/debianmaster                                              Deferred node password secret validation complete
kube-system            34m         Normal    Created                             pod/traefik-5cbdcf97f4-bbnx7                                   Created container: traefik
kube-system            34m         Normal    Created                             pod/local-path-provisioner-5b5f758bcf-7hq92                    Created container: local-path-provisioner
kube-system            34m         Normal    Pulled                              pod/local-path-provisioner-5b5f758bcf-7hq92                    Container image "rancher/local-path-provisioner:v0.0.31" already present on machine
kube-system            34m         Normal    Started                             pod/coredns-ccb96694c-2mrz9                                    Started container coredns
kube-system            34m         Normal    Started                             pod/local-path-provisioner-5b5f758bcf-7hq92                    Started container local-path-provisioner
argocd                 34m         Normal    Pulling                             pod/argocd-applicationset-controller-75c59fc666-bkzbt          Pulling image "quay.io/argoproj/argocd:v3.1.1"
default                34m         Normal    Pulled                              pod/hello-world-795988dcf6-gjhr9                               Container image "gcr.io/google-samples/hello-app:1.0" already present on machine
pauhome                34m         Normal    Created                             pod/web-595c7c58b5-mcb6x                                       Created container: web
kube-system            34m         Normal    Started                             pod/traefik-5cbdcf97f4-bbnx7                                   Started container traefik
default                34m         Normal    Pulling                             pod/nginx-676b6c5bbc-kcg8n                                     Pulling image "nginx"
pauhome                34m         Normal    Pulled                              pod/web-595c7c58b5-mcb6x                                       Container image "nginx:stable" already present on machine
pauhome                34m         Normal    Pulled                              pod/web-595c7c58b5-n4kjr                                       Container image "nginx:stable" already present on machine
argocd                 34m         Normal    Pulled                              pod/argocd-repo-server-95c9d69c6-2vxj4                         Container image "quay.io/argoproj/argocd:v3.1.1" already present on machine
pauhome                34m         Normal    Created                             pod/web-595c7c58b5-n4kjr                                       Created container: web
pauhome                34m         Normal    Started                             pod/web-595c7c58b5-n4kjr                                       Started container web
default                34m         Normal    Started                             pod/hello-world-795988dcf6-gjhr9                               Started container hello-app
pauhome                34m         Normal    Started                             pod/web-595c7c58b5-mcb6x                                       Started container web
default                34m         Normal    Created                             pod/hello-world-795988dcf6-gjhr9                               Created container: hello-app
argocd                 34m         Normal    Pulled                              pod/argocd-applicationset-controller-75c59fc666-bkzbt          Successfully pulled image "quay.io/argoproj/argocd:v3.1.1" in 1.354s (1.354s including waiting). Image size: 191031950 bytes.
argocd                 34m         Normal    Created                             pod/argocd-applicationset-controller-75c59fc666-bkzbt          Created container: argocd-applicationset-controller
argocd                 34m         Normal    Started                             pod/argocd-applicationset-controller-75c59fc666-bkzbt          Started container argocd-applicationset-controller
argocd                 34m         Normal    Created                             pod/argocd-repo-server-95c9d69c6-2vxj4                         Created container: copyutil
argocd                 34m         Normal    Started                             pod/argocd-repo-server-95c9d69c6-2vxj4                         Started container copyutil
argocd                 34m         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.25]
kube-system            34m         Normal    ApplyingManifest                    addon/coredns                                                  Applying manifest at "/var/lib/rancher/k3s/server/manifests/coredns.yaml"
kube-system            34m         Normal    AppliedManifest                     addon/ccm                                                      Applied manifest at "/var/lib/rancher/k3s/server/manifests/ccm.yaml"
kube-system            34m         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.25]
kube-system            34m         Normal    ApplyJob                            helmchart/traefik                                              Applying HelmChart using Job kube-system/helm-install-traefik
kube-system            34m         Normal    AppliedDaemonSet                    service/traefik                                                Applied LoadBalancer DaemonSet kube-system/svclb-traefik-cd2bf767
kube-system            34m         Normal    EnsuringLoadBalancer                service/traefik                                                Ensuring load balancer
kube-system            34m         Normal    ApplyJob                            helmchart/traefik-crd                                          Applying HelmChart using Job kube-system/helm-install-traefik-crd
argocd                 34m         Normal    EnsuringLoadBalancer                service/argocd-server                                          Ensuring load balancer
argocd                 34m         Normal    AppliedDaemonSet                    service/argocd-server                                          Applied LoadBalancer DaemonSet kube-system/svclb-argocd-server-28f86d56
kube-system            34m         Normal    Pulled                              pod/headlamp-666485dd85-hpxln                                  Successfully pulled image "ghcr.io/headlamp-k8s/headlamp:latest" in 22.563s (22.563s including waiting). Image size: 98254074 bytes.
kube-system            34m         Warning   Unhealthy                           pod/traefik-5cbdcf97f4-bbnx7                                   Readiness probe failed: Get "http://10.42.2.171:9000/ping": dial tcp 10.42.2.171:9000: connect: connection refused
argocd                 34m         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.25] -> [192.168.0.18 192.168.0.25]
kube-system            34m         Warning   Unhealthy                           pod/traefik-5cbdcf97f4-bbnx7                                   Readiness probe failed: HTTP probe failed with statuscode: 404
kube-system            34m         Normal    AppliedManifest                     addon/coredns                                                  Applied manifest at "/var/lib/rancher/k3s/server/manifests/coredns.yaml"
kube-system            34m         Normal    ApplyingManifest                    addon/local-storage                                            Applying manifest at "/var/lib/rancher/k3s/server/manifests/local-storage.yaml"
kube-system            34m         Warning   Unhealthy                           pod/traefik-5cbdcf97f4-bbnx7                                   Liveness probe failed: HTTP probe failed with statuscode: 404
kube-system            34m         Normal    Started                             pod/headlamp-666485dd85-hpxln                                  Started container headlamp
kube-system            34m         Normal    Created                             pod/headlamp-666485dd85-hpxln                                  Created container: headlamp
default                34m         Normal    RegisteredNode                      node/debianworker2                                             Node debianworker2 event: Registered Node debianworker2 in Controller
default                34m         Normal    RegisteredNode                      node/debianmaster                                              Node debianmaster event: Registered Node debianmaster in Controller
default                34m         Normal    RegisteredNode                      node/debianworker1                                             Node debianworker1 event: Registered Node debianworker1 in Controller
monitoring             34m         Normal    TaintManagerEviction                pod/loki-0                                                     Cancelling deletion of Pod monitoring/loki-0
monitoring             34m         Normal    TaintManagerEviction                pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Cancelling deletion of Pod monitoring/prometheus-monitorgrafana-kube-promet-prometheus-0
argocd                 34m         Normal    TaintManagerEviction                pod/argocd-application-controller-0                            Cancelling deletion of Pod argocd/argocd-application-controller-0
kube-system            34m         Normal    AppliedManifest                     addon/aggregated-metrics-reader                                Applied manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
kube-system            34m         Normal    ApplyingManifest                    addon/aggregated-metrics-reader                                Applying manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
argocd                 34m         Normal    Pulling                             pod/argocd-redis-65b9d6c547-xqlsp                              Pulling image "public.ecr.aws/docker/library/redis:7.2.7-alpine"
kube-system            34m         Normal    AppliedManifest                     addon/local-storage                                            Applied manifest at "/var/lib/rancher/k3s/server/manifests/local-storage.yaml"
argocd                 34m         Normal    Pulled                              pod/argocd-dex-server-55bdf84957-lbcgk                         Successfully pulled image "ghcr.io/dexidp/dex:v2.43.0" in 766ms (766ms including waiting). Image size: 43032839 bytes.
kube-system            34m         Normal    ApplyingManifest                    addon/metrics-server-deployment                                Applying manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
kube-system            34m         Normal    ApplyingManifest                    addon/auth-reader                                              Applying manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
argocd                 34m         Normal    Pulling                             pod/argocd-repo-server-95c9d69c6-2vxj4                         Pulling image "quay.io/argoproj/argocd:v3.1.1"
kube-system            34m         Normal    ApplyingManifest                    addon/metrics-apiservice                                       Applying manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
kube-system            34m         Normal    AppliedManifest                     addon/auth-delegator                                           Applied manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
kube-system            34m         Normal    AppliedManifest                     addon/metrics-apiservice                                       Applied manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
kube-system            34m         Normal    AppliedManifest                     addon/auth-reader                                              Applied manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
kube-system            34m         Normal    ApplyingManifest                    addon/auth-delegator                                           Applying manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
argocd                 34m         Normal    Pulling                             pod/argocd-dex-server-55bdf84957-lbcgk                         Pulling image "ghcr.io/dexidp/dex:v2.43.0"
monitoring             34m         Normal    Pulled                              pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Container image "quay.io/prometheus/alertmanager:v0.28.1" already present on machine
kube-system            34m         Normal    AppliedManifest                     addon/metrics-server-deployment                                Applied manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
monitoring             34m         Normal    Created                             pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Created container: alertmanager
argocd                 34m         Normal    Pulled                              pod/argocd-redis-65b9d6c547-xqlsp                              Successfully pulled image "public.ecr.aws/docker/library/redis:7.2.7-alpine" in 896ms (896ms including waiting). Image size: 16844051 bytes.
argocd                 34m         Normal    Created                             pod/argocd-redis-65b9d6c547-xqlsp                              Created container: redis
kube-system            34m         Normal    ApplyingManifest                    addon/metrics-server-service                                   Applying manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
argocd                 34m         Normal    Started                             pod/argocd-redis-65b9d6c547-xqlsp                              Started container redis
argocd                 34m         Normal    Pulled                              pod/argocd-repo-server-95c9d69c6-2vxj4                         Successfully pulled image "quay.io/argoproj/argocd:v3.1.1" in 977ms (977ms including waiting). Image size: 191031950 bytes.
monitoring             34m         Normal    Created                             pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Created container: config-reloader
monitoring             34m         Normal    Pulled                              pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0" already present on machine
kube-system            34m         Normal    AppliedManifest                     addon/resource-reader                                          Applied manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
kube-system            34m         Normal    ApplyingManifest                    addon/resource-reader                                          Applying manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
kube-system            34m         Normal    AppliedManifest                     addon/metrics-server-service                                   Applied manifest at "/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
monitoring             34m         Normal    Started                             pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Started container alertmanager
kube-system            34m         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.25] -> [192.168.0.25]
argocd                 34m         Normal    Started                             pod/argocd-dex-server-55bdf84957-lbcgk                         Started container dex
argocd                 34m         Normal    Created                             pod/argocd-dex-server-55bdf84957-lbcgk                         Created container: dex
kube-system            34m         Normal    ApplyingManifest                    addon/rolebindings                                             Applying manifest at "/var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
default                34m         Normal    NodeNotReady                        node/debianworker2                                             Node debianworker2 status is now: NodeNotReady
default                34m         Warning   Rebooted                            node/debianworker2                                             Node debianworker2 has been rebooted, boot id: a52a5dff-45f3-4c48-b9bc-c00331f87503
default                34m         Warning   PossibleMemoryBackedVolumesOnDisk   node/debianworker2                                             The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
argocd                 34m         Normal    Created                             pod/argocd-repo-server-95c9d69c6-2vxj4                         Created container: argocd-repo-server
default                34m         Warning   InvalidDiskCapacity                 node/debianworker2                                             invalid capacity 0 on image filesystem
argocd                 34m         Normal    Started                             pod/argocd-repo-server-95c9d69c6-2vxj4                         Started container argocd-repo-server
default                34m         Normal    Starting                            node/debianworker2                                             Starting kubelet.
default                34m         Normal    CertificateExpirationOK             node/debianworker2                                             Node and Certificate Authority certificates managed by k3s are OK
monitoring             34m         Normal    Started                             pod/alertmanager-monitorgrafana-kube-promet-alertmanager-0     Started container config-reloader
kube-system            34m         Normal    AppliedManifest                     addon/rolebindings                                             Applied manifest at "/var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
default                34m         Normal    NodeAllocatableEnforced             node/debianworker2                                             Updated Node Allocatable limit across pods
monitoring             34m         Normal    SandboxChanged                      pod/monitorgrafana-prometheus-node-exporter-m96w7              Pod sandbox changed, it will be killed and re-created.
default                34m         Normal    NodeReady                           node/debianworker2                                             Node debianworker2 status is now: NodeReady
kube-system            34m         Warning   FailedCreatePodSandBox              pod/svclb-traefik-cd2bf767-7pmqm                               Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "c719bb071a74351ffb62b16229e10d1bbfd021fb53b2088869632b41dcd9d01f": plugin type="flannel" failed (add): loadFlannelSubnetEnv failed: open /run/flannel/subnet.env: no such file or directory
monitoring             34m         Warning   FailedMount                         pod/loki-0                                                     MountVolume.SetUp failed for volume "config" : object "monitoring"/"loki" not registered
kube-system            34m         Warning   FailedCreatePodSandBox              pod/svclb-argocd-server-28f86d56-rgqct                         Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "eefb7dc294135455cc0c43d14d0b75a8c6617a23f724e83ceefe1645518e179f": plugin type="flannel" failed (add): loadFlannelSubnetEnv failed: open /run/flannel/subnet.env: no such file or directory
monitoring             34m         Normal    Pulled                              pod/monitorgrafana-prometheus-node-exporter-m96w7              Container image "quay.io/prometheus/node-exporter:v1.9.1" already present on machine
monitoring             34m         Warning   FailedMount                         pod/prometheus-monitorgrafana-kube-promet-prometheus-0         MountVolume.SetUp failed for volume "tls-assets" : object "monitoring"/"prometheus-monitorgrafana-kube-promet-prometheus-tls-assets-0" not registered
monitoring             34m         Warning   FailedMount                         pod/prometheus-monitorgrafana-kube-promet-prometheus-0         MountVolume.SetUp failed for volume "config" : object "monitoring"/"prometheus-monitorgrafana-kube-promet-prometheus" not registered
monitoring             34m         Warning   FailedMount                         pod/prometheus-monitorgrafana-kube-promet-prometheus-0         MountVolume.SetUp failed for volume "prometheus-monitorgrafana-kube-promet-prometheus-rulefiles-0" : object "monitoring"/"prometheus-monitorgrafana-kube-promet-prometheus-rulefiles-0" not registered
default                34m         Normal    NodeHasNoDiskPressure               node/debianworker2                                             Node debianworker2 status is now: NodeHasNoDiskPressure
default                34m         Normal    NodeHasSufficientMemory             node/debianworker2                                             Node debianworker2 status is now: NodeHasSufficientMemory
default                34m         Normal    NodeHasSufficientPID                node/debianworker2                                             Node debianworker2 status is now: NodeHasSufficientPID
monitoring             34m         Warning   FailedMount                         pod/prometheus-monitorgrafana-kube-promet-prometheus-0         MountVolume.SetUp failed for volume "web-config" : object "monitoring"/"prometheus-monitorgrafana-kube-promet-prometheus-web-config" not registered
argocd                 34m         Warning   FailedMount                         pod/argocd-application-controller-0                            MountVolume.SetUp failed for volume "argocd-repo-server-tls" : object "argocd"/"argocd-repo-server-tls" not registered
kube-system            34m         Normal    ApplyingManifest                    addon/runtimes                                                 Applying manifest at "/var/lib/rancher/k3s/server/manifests/runtimes.yaml"
argocd                 34m         Warning   FailedMount                         pod/argocd-application-controller-0                            MountVolume.SetUp failed for volume "argocd-cmd-params-cm" : object "argocd"/"argocd-cmd-params-cm" not registered
monitoring             34m         Normal    Started                             pod/monitorgrafana-prometheus-node-exporter-m96w7              Started container node-exporter
kube-system            34m         Normal    AppliedManifest                     addon/runtimes                                                 Applied manifest at "/var/lib/rancher/k3s/server/manifests/runtimes.yaml"
argocd                 34m         Warning   FailedMount                         pod/argocd-application-controller-0                            MountVolume.SetUp failed for volume "kube-api-access-j7vtz" : object "argocd"/"kube-root-ca.crt" not registered
monitoring             34m         Normal    Started                             pod/loki-promtail-k7tfg                                        Started container promtail
monitoring             34m         Normal    Created                             pod/monitorgrafana-prometheus-node-exporter-m96w7              Created container: node-exporter
monitoring             34m         Normal    SandboxChanged                      pod/loki-promtail-k7tfg                                        Pod sandbox changed, it will be killed and re-created.
monitoring             34m         Normal    Pulled                              pod/loki-promtail-k7tfg                                        Container image "docker.io/grafana/promtail:2.9.3" already present on machine
monitoring             34m         Normal    Created                             pod/loki-promtail-k7tfg                                        Created container: promtail
kube-system            34m         Normal    AppliedManifest                     addon/traefik                                                  Applied manifest at "/var/lib/rancher/k3s/server/manifests/traefik.yaml"
kube-system            34m         Normal    ApplyingManifest                    addon/traefik                                                  Applying manifest at "/var/lib/rancher/k3s/server/manifests/traefik.yaml"
argocd                 34m         Normal    SuccessfulCreate                    statefulset/argocd-application-controller                      create Pod argocd-application-controller-0 in StatefulSet argocd-application-controller successful
monitoring             34m         Normal    SuccessfulCreate                    statefulset/loki                                               create Pod loki-0 in StatefulSet loki successful
monitoring             34m         Normal    Started                             pod/loki-0                                                     Started container loki
monitoring             34m         Normal    SuccessfulCreate                    statefulset/prometheus-monitorgrafana-kube-promet-prometheus   create Pod prometheus-monitorgrafana-kube-promet-prometheus-0 in StatefulSet prometheus-monitorgrafana-kube-promet-prometheus successful
argocd                 34m         Normal    Pulling                             pod/argocd-application-controller-0                            Pulling image "quay.io/argoproj/argocd:v3.1.1"
monitoring             34m         Warning   Unhealthy                           pod/monitorgrafana-677d8d6465-tktwr                            Readiness probe failed: Get "http://10.42.2.155:3000/api/health": dial tcp 10.42.2.155:3000: connect: connection refused
kube-system            34m         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.25] -> [192.168.0.18]
monitoring             34m         Normal    Pulled                              pod/loki-0                                                     Container image "grafana/loki:2.6.1" already present on machine
monitoring             34m         Normal    Created                             pod/loki-0                                                     Created container: loki
argocd                 34m         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.18]
monitoring             34m         Normal    Pulled                              pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0" already present on machine
monitoring             34m         Normal    Created                             pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Created container: init-config-reloader
monitoring             34m         Normal    Started                             pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Started container init-config-reloader
argocd                 34m         Normal    Created                             pod/argocd-application-controller-0                            Created container: argocd-application-controller
kube-system            34m         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18] -> [192.168.0.18]
argocd                 34m         Normal    Pulled                              pod/argocd-application-controller-0                            Successfully pulled image "quay.io/argoproj/argocd:v3.1.1" in 799ms (799ms including waiting). Image size: 191031950 bytes.
argocd                 34m         Normal    Started                             pod/argocd-application-controller-0                            Started container argocd-application-controller
monitoring             34m         Normal    Pulled                              pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Container image "quay.io/prometheus/prometheus:v3.5.0" already present on machine
monitoring             34m         Normal    Created                             pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Created container: prometheus
monitoring             34m         Normal    Pulled                              pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0" already present on machine
monitoring             34m         Normal    Started                             pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Started container prometheus
monitoring             34m         Normal    Started                             pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Started container config-reloader
monitoring             34m         Normal    Created                             pod/prometheus-monitorgrafana-kube-promet-prometheus-0         Created container: config-reloader
kube-system            33m         Normal    SandboxChanged                      pod/svclb-argocd-server-28f86d56-rgqct                         Pod sandbox changed, it will be killed and re-created.
kube-system            33m         Normal    Pulled                              pod/svclb-argocd-server-28f86d56-rgqct                         Container image "rancher/klipper-lb:v0.4.10" already present on machine
kube-system            33m         Normal    Started                             pod/svclb-argocd-server-28f86d56-rgqct                         Started container lb-tcp-8082
kube-system            33m         Normal    Created                             pod/svclb-argocd-server-28f86d56-rgqct                         Created container: lb-tcp-8082
argocd                 33m         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18] -> [192.168.0.18 192.168.0.25]
default                33m         Normal    Started                             pod/nginx-676b6c5bbc-kcg8n                                     Started container nginx
default                33m         Normal    Created                             pod/nginx-676b6c5bbc-kcg8n                                     Created container: nginx
default                33m         Normal    Pulled                              pod/nginx-676b6c5bbc-kcg8n                                     Successfully pulled image "nginx" in 26.614s (26.618s including waiting). Image size: 62870438 bytes.
cyclops                33m         Warning   BackOff                             pod/cyclops-ui-97ffc847c-qz262                                 Back-off restarting failed container cyclops-ui in pod cyclops-ui-97ffc847c-qz262_cyclops(62e78197-def2-461b-a96a-9c22359dc725)
argocd                 33m         Warning   Unhealthy                           pod/argocd-server-84477d5958-z5nwc                             Readiness probe failed: Get "http://10.42.2.165:8888/healthz": dial tcp 10.42.2.165:8888: connect: connection refused
kube-system            33m         Normal    SandboxChanged                      pod/svclb-traefik-cd2bf767-7pmqm                               Pod sandbox changed, it will be killed and re-created.
kube-system            33m         Normal    Pulled                              pod/svclb-traefik-cd2bf767-7pmqm                               Container image "rancher/klipper-lb:v0.4.10" already present on machine
kube-system            33m         Normal    Created                             pod/svclb-traefik-cd2bf767-7pmqm                               Created container: lb-tcp-80
kube-system            33m         Normal    Pulled                              pod/svclb-traefik-cd2bf767-7pmqm                               Container image "rancher/klipper-lb:v0.4.10" already present on machine
kube-system            33m         Normal    Created                             pod/svclb-traefik-cd2bf767-7pmqm                               Created container: lb-tcp-443
kube-system            33m         Normal    Started                             pod/svclb-traefik-cd2bf767-7pmqm                               Started container lb-tcp-80
kube-system            33m         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18] -> [192.168.0.18 192.168.0.25]
kube-system            33m         Normal    Started                             pod/svclb-traefik-cd2bf767-7pmqm                               Started container lb-tcp-443
cyclops                33m         Normal    Created                             pod/cyclops-ui-97ffc847c-qz262                                 Created container: cyclops-ui
cyclops                33m         Normal    Pulled                              pod/cyclops-ui-97ffc847c-qz262                                 Container image "cyclopsui/cyclops-ui:v0.21.1" already present on machine
cyclops                33m         Normal    Started                             pod/cyclops-ui-97ffc847c-qz262                                 Started container cyclops-ui
monitoring             33m         Warning   Unhealthy                           pod/loki-promtail-k7tfg                                        Readiness probe failed: HTTP probe failed with statuscode: 500
monitoring             33m         Normal    Pulled                              pod/monitorgrafana-677d8d6465-tktwr                            Container image "docker.io/grafana/grafana:12.1.0" already present on machine
monitoring             33m         Normal    Created                             pod/monitorgrafana-677d8d6465-tktwr                            Created container: grafana
monitoring             33m         Warning   Unhealthy                           pod/loki-0                                                     Readiness probe failed: HTTP probe failed with statuscode: 503
monitoring             33m         Warning   Unhealthy                           pod/loki-0                                                     Liveness probe failed: HTTP probe failed with statuscode: 503
argocd                 32m         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.18 192.168.0.25]
kube-system            32m         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.18 192.168.0.25]
monitoring             32m         Warning   Unhealthy                           pod/loki-promtail-5dr6s                                        Readiness probe failed: Get "http://10.42.2.167:3101/ready": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
default                32m         Normal    TaintManagerEviction                pod/testnet                                                    Cancelling deletion of Pod default/testnet
argocd                 31m         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.18 192.168.0.24 192.168.0.25]
kube-system            31m         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.25] -> [192.168.0.18 192.168.0.24 192.168.0.25]
argocd                 30m         Normal    UpdatedLoadBalancer                 service/argocd-server                                          Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.24 192.168.0.25] -> [192.168.0.18 192.168.0.24 192.168.0.25]
kube-system            30m         Normal    UpdatedLoadBalancer                 service/traefik                                                Updated LoadBalancer with new IPs: [192.168.0.18 192.168.0.24 192.168.0.25] -> [192.168.0.18 192.168.0.24 192.168.0.25]
default                19m         Warning   InvalidDiskCapacity                 node/debianworker2                                             invalid capacity 0 on image filesystem
default                19m         Warning   PossibleMemoryBackedVolumesOnDisk   node/debianworker2                                             The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
default                19m         Normal    CertificateExpirationOK             node/debianworker2                                             Node and Certificate Authority certificates managed by k3s are OK
default                19m         Normal    Starting                            node/debianworker2                                             Starting kubelet.
default                19m         Normal    NodeAllocatableEnforced             node/debianworker2                                             Updated Node Allocatable limit across pods
default                19m         Normal    NodeHasSufficientPID                node/debianworker2                                             Node debianworker2 status is now: NodeHasSufficientPID
default                19m         Normal    NodeHasNoDiskPressure               node/debianworker2                                             Node debianworker2 status is now: NodeHasNoDiskPressure
default                19m         Normal    NodeHasSufficientMemory             node/debianworker2                                             Node debianworker2 status is now: NodeHasSufficientMemory
default                16m         Warning   InvalidDiskCapacity                 node/debianworker1                                             invalid capacity 0 on image filesystem
default                16m         Normal    NodeHasSufficientMemory             node/debianworker1                                             Node debianworker1 status is now: NodeHasSufficientMemory
default                16m         Warning   PossibleMemoryBackedVolumesOnDisk   node/debianworker1                                             The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
default                16m         Normal    Starting                            node/debianworker1                                             Starting kubelet.
default                16m         Normal    CertificateExpirationOK             node/debianworker1                                             Node and Certificate Authority certificates managed by k3s are OK
default                16m         Normal    NodeHasSufficientPID                node/debianworker1                                             Node debianworker1 status is now: NodeHasSufficientPID
default                16m         Normal    NodeHasNoDiskPressure               node/debianworker1                                             Node debianworker1 status is now: NodeHasNoDiskPressure
default                16m         Normal    NodeAllocatableEnforced             node/debianworker1                                             Updated Node Allocatable limit across pods
argocd                 9m32s       Warning   Unhealthy                           pod/argocd-server-84477d5958-z5nwc                             Liveness probe failed: Get "http://10.42.2.165:8888/healthz?full=true": dial tcp 10.42.2.165:8888: connect: connection refused
argocd                 4m38s       Warning   BackOff                             pod/argocd-server-84477d5958-z5nwc                             Back-off restarting failed container argocd-server in pod argocd-server-84477d5958-z5nwc_argocd(17f73213-6a33-4711-a57e-16b3436732fc)
monitoring             4m29s       Warning   BackOff                             pod/monitorgrafana-677d8d6465-tktwr                            Back-off restarting failed container grafana in pod monitorgrafana-677d8d6465-tktwr_monitoring(70ce476e-785e-4363-a2e4-69a553b0c7b7)
default                3m51s       Normal    NodeAllocatableEnforced             node/debianworker2                                             Updated Node Allocatable limit across pods
default                3m51s       Warning   PossibleMemoryBackedVolumesOnDisk   node/debianworker2                                             The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
default                3m51s       Normal    NodeHasSufficientPID                node/debianworker2                                             Node debianworker2 status is now: NodeHasSufficientPID
default                3m51s       Normal    NodeHasNoDiskPressure               node/debianworker2                                             Node debianworker2 status is now: NodeHasNoDiskPressure
default                3m51s       Normal    CertificateExpirationOK             node/debianworker2                                             Node and Certificate Authority certificates managed by k3s are OK
default                3m51s       Normal    Starting                            node/debianworker2                                             Starting kubelet.
default                3m51s       Normal    NodeHasSufficientMemory             node/debianworker2                                             Node debianworker2 status is now: NodeHasSufficientMemory
default                104s        Normal    NodeAllocatableEnforced             node/debianworker1                                             Updated Node Allocatable limit across pods
default                104s        Normal    NodeHasSufficientMemory             node/debianworker1                                             Node debianworker1 status is now: NodeHasSufficientMemory
default                104s        Warning   InvalidDiskCapacity                 node/debianworker1                                             invalid capacity 0 on image filesystem
default                104s        Normal    NodeHasSufficientPID                node/debianworker1                                             Node debianworker1 status is now: NodeHasSufficientPID
default                104s        Normal    CertificateExpirationOK             node/debianworker1                                             Node and Certificate Authority certificates managed by k3s are OK
default                104s        Warning   PossibleMemoryBackedVolumesOnDisk   node/debianworker1                                             The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
default                104s        Normal    NodeHasNoDiskPressure               node/debianworker1                                             Node debianworker1 status is now: NodeHasNoDiskPressure
default                104s        Normal    Starting                            node/debianworker1                                             Starting kubelet.
default                <invalid>   Normal    NodeHasSufficientPID                node/debianworker1                                             Node debianworker1 status is now: NodeHasSufficientPID
default                <invalid>   Normal    Starting                            node/debianworker1                                             Starting kubelet.
default                <invalid>   Normal    NodeHasNoDiskPressure               node/debianworker1                                             Node debianworker1 status is now: NodeHasNoDiskPressure
default                <invalid>   Normal    NodeHasSufficientMemory             node/debianworker1                                             Node debianworker1 status is now: NodeHasSufficientMemory
default                <invalid>   Warning   Rebooted                            node/debianworker1                                             Node debianworker1 has been rebooted, boot id: cf36ea7a-1e16-4797-a011-c406b7102cff
default                <invalid>   Normal    CertificateExpirationOK             node/debianworker1                                             Node and Certificate Authority certificates managed by k3s are OK
default                <invalid>   Normal    NodeNotReady                        node/debianworker1                                             Node debianworker1 status is now: NodeNotReady
default                <invalid>   Normal    NodeAllocatableEnforced             node/debianworker1                                             Updated Node Allocatable limit across pods
default                <invalid>   Warning   PossibleMemoryBackedVolumesOnDisk   node/debianworker1                                             The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
default                <invalid>   Warning   InvalidDiskCapacity                 node/debianworker1                                             invalid capacity 0 on image filesystem
default                <invalid>   Normal    NodeReady                           node/debianworker1                                             Node debianworker1 status is now: NodeReady
default                <invalid>   Warning   FailedMount                         pod/testnet                                                    MountVolume.SetUp failed for volume "kube-api-access-cbths" : object "default"/"kube-root-ca.crt" not registered
monitoring             <invalid>   Normal    SandboxChanged                      pod/monitorgrafana-prometheus-node-exporter-2hhbs              Pod sandbox changed, it will be killed and re-created.
kube-system            <invalid>   Warning   FailedCreatePodSandBox              pod/svclb-argocd-server-28f86d56-qr66h                         Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "30eb49c905410a37faf949f58c7ae100a12d54799680927b0ff43de5b7667acc": plugin type="flannel" failed (add): loadFlannelSubnetEnv failed: open /run/flannel/subnet.env: no such file or directory
monitoring             <invalid>   Warning   FailedCreatePodSandBox              pod/loki-promtail-9g6nk                                        Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "73dbe4284c18a31b68b04dcc638abd36c6579883045523b44f9a457a3d5ff746": plugin type="flannel" failed (add): loadFlannelSubnetEnv failed: open /run/flannel/subnet.env: no such file or directory
monitoring             <invalid>   Normal    Started                             pod/monitorgrafana-prometheus-node-exporter-2hhbs              Started container node-exporter
monitoring             <invalid>   Normal    Pulled                              pod/monitorgrafana-prometheus-node-exporter-2hhbs              Container image "quay.io/prometheus/node-exporter:v1.9.1" already present on machine
monitoring             <invalid>   Normal    Created                             pod/monitorgrafana-prometheus-node-exporter-2hhbs              Created container: node-exporter
kube-system            <invalid>   Warning   FailedCreatePodSandBox              pod/svclb-traefik-cd2bf767-68vkn                               Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "eaf8ce1e4c1b9de5d235e8a30fb612853c7e2dfac4050cf88ad54a08aceeedcd": plugin type="flannel" failed (add): loadFlannelSubnetEnv failed: open /run/flannel/subnet.env: no such file or directory
monitoring             <invalid>   Normal    Pulled                              pod/loki-promtail-9g6nk                                        Container image "docker.io/grafana/promtail:2.9.3" already present on machine
monitoring             <invalid>   Normal    Started                             pod/loki-promtail-9g6nk                                        Started container promtail
monitoring             <invalid>   Normal    SandboxChanged                      pod/loki-promtail-9g6nk                                        Pod sandbox changed, it will be killed and re-created.
monitoring             <invalid>   Normal    Created                             pod/loki-promtail-9g6nk                                        Created container: promtail
kube-system            <invalid>   Normal    SandboxChanged                      pod/svclb-argocd-server-28f86d56-qr66h                         Pod sandbox changed, it will be killed and re-created.
kube-system            <invalid>   Normal    Created                             pod/svclb-argocd-server-28f86d56-qr66h                         Created container: lb-tcp-8082
kube-system            <invalid>   Normal    Started                             pod/svclb-argocd-server-28f86d56-qr66h                         Started container lb-tcp-8082
kube-system            <invalid>   Normal    Pulled                              pod/svclb-argocd-server-28f86d56-qr66h                         Container image "rancher/klipper-lb:v0.4.10" already present on machine
kube-system            <invalid>   Normal    SandboxChanged                      pod/svclb-traefik-cd2bf767-68vkn                               Pod sandbox changed, it will be killed and re-created.
kube-system            <invalid>   Normal    Pulled                              pod/svclb-traefik-cd2bf767-68vkn                               Container image "rancher/klipper-lb:v0.4.10" already present on machine
kube-system            <invalid>   Normal    Created                             pod/svclb-traefik-cd2bf767-68vkn                               Created container: lb-tcp-80
kube-system            <invalid>   Normal    Started                             pod/svclb-traefik-cd2bf767-68vkn                               Started container lb-tcp-80
kube-system            <invalid>   Normal    Pulled                              pod/svclb-traefik-cd2bf767-68vkn                               Container image "rancher/klipper-lb:v0.4.10" already present on machine
kube-system            <invalid>   Normal    Created                             pod/svclb-traefik-cd2bf767-68vkn                               Created container: lb-tcp-443
kube-system            <invalid>   Normal    Started                             pod/svclb-traefik-cd2bf767-68vkn                               Started container lb-tcp-443
